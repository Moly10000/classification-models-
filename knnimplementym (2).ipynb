{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "YULIANA ALEJANDRA MOLINA CORTES\n",
        "9B ROBOTICS ENGINEERING\n",
        "TEACHER VICTOR ALEJANDRO ORTIZ SANTIAGO\n",
        "UPY MACHINE LEARNIGN\n",
        "KNN IMPLEMENTATION"
      ],
      "metadata": {
        "id": "44paiDn_yY4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "k-nn implementation\n",
        "\n",
        "k-NN or K Nearest Neighbors is a widelly known machine learning supervised algorithm that, as its name suggests,\n",
        "is based on the proximity of the nearest neighbors to classify or make predictions about an individual unkown data point.\n",
        "It can be understood as objects that are similar and thus, belong to the same category, or have resembling output values.\n",
        "To do this, K-NN takes an unknown sample and finds the K samples that have the nearest distance from it,\n",
        "with K being a predefined positive number. It then assigns the most common class among them to the unknown sample,\n",
        "or it calculates the weighted value of the K's to predict the value of the unknown data point (in the case of regression).\n",
        "\n",
        "k-NN is a flexible model because it can find desision boundaries of any shape between classes,\n",
        "and can be used for both classificacion and regression problems.\n",
        "\n",
        "An example is determining if a person in a classroom is a man or a woman,\n",
        "depending on the genders of the surrounding people.\n",
        "If K, which is the number of neighbors, is set to K=5, then the gender of the unknown person will depend on\n",
        "the genders of the surrounding 5 nearest persons. So, if this person is surrounded by 5 nearest neighbors who are women,\n",
        "they will be classified as a woman. If the person is surrounded by 5 nearest neighbors that are men, following the algorithm,\n",
        "the person will be classified as a man."
      ],
      "metadata": {
        "id": "2PdH-HWwyY4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pseudocode for the k NN Kernel:\n",
        "\n",
        "\n",
        "The kNN kernel algorithm utilizes one dimensional block and thread structure. Each thread works on a row of the chunk and identifies the k -nearest neighbors for each respective row index. Additionally, based on position of the chunk, it skips certain indices, e.g., the diagonal distance values (i.e., the distance from the point itself) and values that fall into the extra (padding) regions of the matrix.\n",
        "\n",
        "Function KNN(classify or predict, training_data, unknown_sample, K):\n",
        "    Calculate the distance between unknown_sample and all points in training_data\n",
        "    Select the K nearest points\n",
        "    If it's a classification task:\n",
        "        Count the classes of the K nearest points\n",
        "        Assign the most common class to the unknown_sample\n",
        "    If it's a regression task:\n",
        "        Calculate the average (or weighted) value of the K nearest points\n",
        "        Assign that value to the unknown_sample\n",
        "    Return the classification or prediction for the unknown_sample\n",
        "\n",
        "It can also be simplified as:\n",
        "INPUT: Training Set D={(x1,y1),...xN,yN}\n",
        "number of nearest neighbors k\n",
        "a distance metric d(x,y)\n",
        "a test sample x\n",
        "\n",
        "for each training sample (xi,yi), the distance between x and xi\n",
        "Let N C_ D be the set of training samples with the k smallest distances d(x,xi)\n",
        "return the majority label of the samples in N"
      ],
      "metadata": {
        "id": "pTXcrnVTyY4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eventhough the kNN does not have loss function or optimization function, the general definitions are:\n",
        "\n",
        "Loss Function:\n",
        "They are helpful to train a neural network. Given an input and a target, they calculate the loss, i.e difference between output and target variable. For Classification loss functions the output variable in classification problem is usually a probability value f(x), called the score for the input x. The score represents the confidence of our prediction. The target variable y, is a binary variable, 1 for true and -1 for false.\n",
        "On an example (x,y), the margin is defined as yf(x). The margin is a measure of how correct we are. Most classification losses mainly aim to maximize the margin.\n",
        "\n",
        "Back Propogation and Optimisation Function: Error J(w) is a function of internal parameters of model i.e weights and bias. For accurate predictions, one needs to minimize the calculated error. In a neural network, this is done using back propagation. The current error is typically propagated backwards to a previous layer, where it is used to modify the weights and bias in such a way that the error is minimized. The weights are modified using a function called Optimization Function.\n",
        "Optimisation functions usually calculate the gradient i.e. the partial derivative of loss function with respect to weights, and the weights are modified in the opposite direction of the calculated gradient. This cycle is repeated until we reach the minima of loss function.\n",
        "\n",
        "The loss function and optimization function in KNN depend on the type of task you are addressing. For classification, the loss function can be accuracy or any other classification metric, and optimization is performed automatically during training. For regression, the loss function could be mean squared error (MSE), and optimization is done by searching for the K nearest neighbors."
      ],
      "metadata": {
        "id": "VZtsrQ0wBkcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of kNN:\n",
        "\n",
        "As KNN is a classification algorithm that predicts the class of a data point based on the majority class among its k-nearest neighbors in a feature space, this code defines a KNN class that can be trained on a dataset, calculate Euclidean distances between data points, and make predictions for new data points by finding their nearest neighbors and selecting the most common class among them.\n",
        "\n",
        "It generates example data, splits it into training and testing sets, and defines a `KNN` class. The class includes the calculation of the Euclidean distance between data points, make predictions for new data points by finding their nearest neighbors, and return the most common class label among those neighbors. After creating a KNN model with k=2, it trains on the training data, and then it makes predictions for the test data. Finally, the code prints out the predictions for each test data point, showing which class the algorithm assigns to each based on their proximity to the training data."
      ],
      "metadata": {
        "id": "-OFGN-osyY4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # for numerical operations\n",
        "from collections import Counter  # For counting label occurrences\n",
        "from sklearn.model_selection import train_test_split  # For data split\n",
        "\n",
        "# example data for demonstration\n",
        "data = np.array([[1, 2], [2, 3], [3, 4], [5, 6]])  # Example feature vectors\n",
        "labels = np.array([0, 0, 1, 1])  # Example labels\n",
        "\n",
        "# Split data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define a class Named KNN for the K-Nearest Neighbors algorithm\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k  # Number of neighbors to consider (default is 3)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X  # Training data (feature vectors)\n",
        "        self.y_train = y  # Training labels\n",
        "\n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        # Calculate the Euclidean distance between two points x1 and x2\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Make predictions for a list of data points in X\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        #  a predction for a single data point x\n",
        "        # Calculate distances between x and all training points\n",
        "        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "        # Get indices of the k nearest neighbors\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "\n",
        "        # Get labels of the k nearest neighbors\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "\n",
        "        # Return the most common label among the k nearest neighbors\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)\n",
        "        return most_common[0][0]\n",
        "\n",
        "# Create a KNN model with k=2\n",
        "model = KNN(k=2)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions for the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Print predictions for each test point\n",
        "for i, pred in enumerate(predictions):\n",
        "    print(f\"Prediction for {X_test[i]}: Class {pred}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "SED-IaLcyY4y",
        "outputId": "b1e31426-e103-451f-ac5a-06086c70db15"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Prediction for [2 3]: Class 0\nPrediction for [5 6]: Class 1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predictions are made by the K-Nearest Neighbors (KNN) algorithm for each data point in the test set. Specifically, the data point and the predicted class label assigned by the KNN algorithm. In this example, the KNN algorithm predicted that the first test data point `[2 3]` belongs to \"Class 0\" and the second test data point `[5 6]` belongs to \"Class 1.\" The specific predictions may vary depending on the random data split and the algorithm's decisions based on the training data."
      ],
      "metadata": {
        "id": "ppyNjxr1yY40"
      }
    }
  ]
}
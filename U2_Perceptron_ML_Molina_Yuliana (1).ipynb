{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MACHINE LEARNING UNIT 2\n",
        "- PERCEPTRON IMPLEMENTATION:\n",
        "- YULIANA ALEJANDRA MOLINA CORTES\n",
        "- COMPUTATIONAL ROBOTICS ENGINERING\n",
        "- 9B, TEACHER VICTOR ORTIZ\n"
      ],
      "metadata": {
        "id": "x7mOMQIqiFif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lsm6pG6AifKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptron\n",
        "\n",
        "Perceptron is a deep learning algorithm that works similarly to the neurons of the human body. It classifies data into two classes and takes a set of inputs to then divide the data according to the given features. For this, it performs a weighted linear combination of these inputs and then applies an activation function to produce a binary output (0 or 1) representing the class to which the data point belongs.\n",
        "\n",
        "For this reazon, perceptron is a great tool to make decisions. The main idea is to use different weights to represent the importance of each input, and that the sum of the values should be greater than a threshold value before making a decision like yes or no (true or false) (0 or 1). For example, if I go to the cinema, I will consider factors such as if i like going in the first place; if I have money, what is the movie and who am going with. All this factors are then multiplied by a factor of importance and then added.The decision will be positive or negative depending on the value obtained after this operation. If the value is greater than a predefined  threshold “a”, called “the bias”, then the decision will be positive. Else, it will be negative. This threshold value modifies the observations required.\n",
        "\n",
        "\n",
        "In summary, the steps for the Perceptron altgoritm are setting a threshold value, multiplying all inputs with its weights, adding up all the results, and activating the output, which depends on the condition of, if a>0 the output = 1, else, if a<0 output = 0.\n",
        "\n",
        "\n",
        "This altgoritm was suggested by Frank Rosenblatt, and it can be explained with a simple example:\n",
        "\n",
        "1. Set a threshold value: Threshold = 1.5\n",
        "2. Multiply all inputs with its weights:\n",
        "\n",
        "x1 * w1 = 1 * 0.7 = 0.7\n",
        "\n",
        "\n",
        "x2 * w2 = 0 * 0.6 = 0\n",
        "\n",
        "\n",
        "x3 * w3 = 1 * 0.5 = 0.5\n",
        "\n",
        "\n",
        "x4 * w4 = 0 * 0.3 = 0\n",
        "\n",
        "\n",
        "x5 * w5 = 1 * 0.4 = 0.4\n",
        "\n",
        "\n",
        "3. Sum all the results: 0.7 + 0 + 0.5 + 0 + 0.4 = 1.6 (The Weighted Sum)\n",
        "\n",
        "\n",
        "4. Activate the Output(activation function)\n",
        "\n",
        "if the result is bigger than the bias, then is true or 1, else is 0 or false.\n",
        "\n",
        "Return true if the sum > 1.6 (\"Yes I will go to the the movies”)\n",
        "\n",
        "\n",
        "\n",
        "Perceptron inputs are called nodes, and they have both: a value and a weight. Each input node has a binary value of 1 or 0  true or false or yes or no.\n",
        "The Activation Function is in charge of mapping the the weighted sum into a binary value of 1 or 0.\n",
        "\n",
        "In the example above, the activation function is simple: (sum > 1.5)\n",
        "\n",
        "It is important to denote that there is a distinction between the bias and the activation function. The bias is a learnable parameter that shifts the overall output of a neuron before it passes through the activation function. This shift allows the neuron to capture specific patterns in the data that might not be captured otherwise. On the other hand, the activation function is a mathematical operation applied to the weighted sum of inputs, introducing non-linearity and determining whether the neuron should \"fire\" or not. Essentially, while the bias affects the neuron's output directly, the activation function introduces non-linearity and decision-making capabilities to the neuron's response to its inputs.\n",
        "\n",
        "Perceptron algorithms have been categorized into two phases: one is a single-layer perceptron, and the other is a multi-layer perceptron. The single-layer perceptron organizes or places neurons in a single layer, whereas the multi-layer perceptron assembles neurons across multiple layers. In the context of the perceptron, each neuron utilizes the inputs and provides a response to groups of neurons. This process continues until it reaches the previous layer.\n"
      ],
      "metadata": {
        "id": "G5ZE2RCxjhI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pseudocode in python\n",
        "    # Initialize weights and bias with small random values.\n",
        "        initialize weights (w) with small random values\n",
        "        initialize bias (b) with a small random value\n",
        "    # Set the learning rate\n",
        "        learning_rate = 0.1\n",
        "    # Define the training loop\n",
        "        for each training example (x, y):\n",
        "    # Calculate the weighted sum of inputs\n",
        "        weighted_sum = sum(w[i] * x[i] for i in range(len(x))) + b\n",
        "    # Apply the activation function (usually a step function)\n",
        "    if weighted_sum > 0:\n",
        "        prediction = 1\n",
        "    else:\n",
        "        prediction = 0\n",
        "    # Compute the error\n",
        "    error = y - prediction\n",
        "\n",
        "    # Update the weights and bias\n",
        "    for i in range(len(w)):\n",
        "        w[i] = w[i] + learning_rate * error * x[i]\n",
        "    \n",
        "    b = b + learning_rate * error\n",
        "\n",
        "    # Repeat the training loop for a fixed number of epochs or until convergence\n"
      ],
      "metadata": {
        "id": "UJGMtFliszHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Own implementation\n",
        "\n",
        "This code evaluates all my nodes to decide wether someone should or shouldn't cut its hair deppending on the result compared with the bias value."
      ],
      "metadata": {
        "id": "t4EEE8p_uKa9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsTSKTzyh6vJ",
        "outputId": "42124527-daa7-4377-c8f2-47d9d01714dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bias 1.8\n",
            "sum 1.6\n",
            "Will I cut my hair? False\n"
          ]
        }
      ],
      "source": [
        "# Define the bias for the perceptron\n",
        "bias = 1.8\n",
        "\n",
        "# Define the input values and weights\n",
        "inputs = [1, 0, 1, 0, 1]\n",
        "weights = [0.7, 0.6, 0.5, 0.8, 0.4]\n",
        "\n",
        "# Initialize a variable to store the weighted sum\n",
        "sum = 0\n",
        "\n",
        "# Calculate the weighted sum of inputs\n",
        "for i in range(len(inputs)):\n",
        "    sum += inputs[i] * weights[i]\n",
        "\n",
        "# Check if the weighted sum is greater than the bias\n",
        "# and display the result\n",
        "result = sum > bias\n",
        "\n",
        "print(\"bias\", bias)\n",
        "print(\"sum\", sum)\n",
        "print(\"Will I cut my hair?\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def activation_function(self, x):\n",
        "        # Activation function (step function)\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        # Calculate the weighted sum of inputs and apply the activation function\n",
        "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
        "        return self.activation_function(weighted_sum)\n",
        "\n",
        "# Pre-trained weights and bias (replace with your own values)\n",
        "pretrained_weights = np.array([0.5, -0.5])\n",
        "pretrained_bias = -0.2\n",
        "\n",
        "# Create a Perceptron with pre-trained weights and bias\n",
        "perceptron = Perceptron(input_size=2, weights=pretrained_weights, bias=pretrained_bias)\n",
        "\n",
        "# New data point for prediction\n",
        "new_data_point = np.array([0.2, 0.3])\n",
        "\n",
        "# Make a prediction for the new data point\n",
        "prediction = perceptron.predict(new_data_point)\n",
        "\n",
        "# Print the prediction\n",
        "print(\"Prediction for new data point; the class is: \", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41vUgHLKjoVE",
        "outputId": "e995ff41-8b55-44f5-9261-659b82a1d032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for new data point; the class is:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss function\n",
        "Normally the perceptron algorithm uses a specific type of loss function and optimization method.\n",
        "\n",
        "The Perceptron Loss Function (hinge loss for binary classification problems) calculates the product of the actual label and the predicted label. If the classification is correct, the product will be positive, and the loss will be zero. If the classification is incorrect, the product will be negative, and the loss will be proportional to the distance from the decision boundary. It is defined as follows:\n",
        "\n",
        "\n",
        "L(w) = max(0, -yi(w • xi + b))\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "\n",
        "L (w) is the loss associated with the weights W.\n",
        "\n",
        "\n",
        "Yi is the actual label of the i-th training example (usually + 1 or - 1 for binary classification).\n",
        "\n",
        "\n",
        "w • xi is the dot product of the weights and the features of the i-th training example.\n",
        "\n",
        "b is the bias term.\n",
        "\n",
        "\n",
        "Another possible aproach is to calculate the summatory of the labels by the training example. The loss is calculated only for misclassified examples and, if the prediciton is correct the loss of the example is zero.\n",
        "\n",
        "Loss(w) = - E (from N to I=1) yiw* Xi\n",
        "\n",
        "E being the total summatory\n",
        "\n",
        "\n",
        "N is the number of training examples\n",
        "\n",
        "\n",
        "Yi is the label of th ei training example\n",
        "\n",
        "\n",
        "Xi is the feature vector of the i trainin example\n",
        "\n",
        "\n",
        "W being the weight vector\n",
        "#Optimization Function\n",
        "\n",
        "The perceptron algorithm iteratively updates the weights and bias until it reaches a point where all training examples are classified correctly or until it converges according to a stopping criterion (such as a fixed number of iterations). The optimization function's primary goal is to adjust the weights and bias to minimize the classification errors and improve the model's accuracy.\n",
        "\n",
        "The Perceptron is a simple supervised learning algorithm used for binary classification. Unlike some more advanced algorithms, the Perceptron does not use a complex optimization function, such as cost functions and optimization algorithms like stochastic gradient descent (SGD) or more advanced ones like Adam or RMSprop.\n",
        "\n",
        "The Perceptron uses an \"error correction learning\" algorithm in which it adjusts the weights and bias to correct classification errors. When an error is made in classification, the weights and bias are updated in a way that corrects the error. For this, it uses a learning rate that controls how much the weights are adjusted in each training step.\n",
        "\n",
        "The training process of the Perceptron is quite simple and follows these steps:\n",
        "\n",
        "1. Initialize the weights and bias with random or zero values.\n",
        "\n",
        "2. Present a training example to the Perceptron and calculate the prediction.\n",
        "\n",
        "3. Evalueate the error Compare the prediction to the actual label and calculate the error.\n",
        "\n",
        "4. Adjust the weights and bias based on the error and the learning rate.\n",
        "\n",
        "5. Repeat the steps for several training examples until a stopping condition is met (a maximum number of epochs or until there are no classification errors).\n",
        "\n",
        "The Perceptron does not use a differentiable cost function, and its training can be seen as a solution search in the weight space to find a solution that correctly classifies the training examples. It is important to note that the Perceptron is suitable for linearly separable classification problems, but it is not suitable for more complex problems that require non-linear decision boundaries. For more complex problems, more advanced algorithms and models are typically used.\n",
        "\n",
        "The Perceptron is a simple supervised learning algorithm used for binary classification. Unlike some more advanced algorithms, the Perceptron does not use a complex optimization function, such as cost functions and optimization algorithms like stochastic gradient descent (SGD) or more advanced ones like Adam or RMSprop.\n",
        "\n",
        "In its most basic form, the Perceptron uses an \"error correction learning\" algorithm in which it adjusts the weights and bias to correct classification errors. When an error is made in classification, the weights and bias are updated in a way that corrects the error. This is done using a learning rate that controls how much the weights are adjusted in each training step.\n",
        "\n",
        "The training process of the Perceptron is quite simple and follows these steps:\n",
        "\n",
        "1. Initialize the weights and bias with random or zero values.\n",
        "\n",
        "2. Present a training example to the Perceptron and calculate the prediction.\n",
        "\n",
        "3. Compare the prediction to the actual label and calculate the error.\n",
        "\n",
        "4. Adjust the weights and bias based on the error and the learning rate.\n",
        "\n",
        "5. Repeat the above steps for several training examples until a stopping condition is met (such as a maximum number of epochs or until there are no classification errors).\n",
        "\n",
        "The Perceptron does not use a differentiable cost function, and its training can be seen as a solution search in the weight space to find a solution that correctly classifies the training examples. It is important to note that the Perceptron is suitable for linearly separable classification problems, but it is not suitable for more complex problems that require non-linear decision boundaries. For more complex problems, more advanced algorithms and models are typically used.\n",
        "\n",
        "\n",
        "Perceptron Learning using Gradient Descent: is an optimization method that finds the minimum of an objective\n",
        "function by incrementally updating its parameters in the negative direction of the derivative of this function.\n",
        "The objective function to be minimized is classication error and the parameters of this function are the weights associated with the inputs β. The gradient descent algorithm updates the weights as follows:\n",
        "\n",
        "Bnew <- Bold - p dErr/dB\n",
        "\n",
        "Where p is the learning rate.\n",
        "\n",
        "The classication error can be dened as the distance of misclassied observations to the decision boundary\n",
        "\n",
        "D(b) = -E_(I E M) yi B^T xi\n",
        "\n",
        "M is the set os misclassified points\n",
        "if xi is misclasified the sign of yi B^t xi will be negative.\n",
        "\n",
        "taking the derivative, we obtain\n",
        "\n",
        "dD/dB = -E_(I E M) yixi\n",
        "\n",
        "That is the same as incrementally updating B for each misclassified point Xi\n",
        "\n",
        "Bnew<-Bold + pE(iEM)yixi\n",
        "\n",
        "the updated formula looks like:\n",
        "\n",
        "\n",
        "Bnew <- Bold - pyixi\n",
        "\n",
        "\n",
        "What happens when it updates is that for misclassied point xi,β should be changed in the direction that makes xi as close as possible to the right side."
      ],
      "metadata": {
        "id": "tx3D72h2un3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vo9yFrlLS6Hy"
      }
    }
  ]
}